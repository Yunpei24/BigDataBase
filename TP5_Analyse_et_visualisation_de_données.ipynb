{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZXpujLg/samytqJs/JBaF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yunpei24/BigDataBase/blob/main/TP5_Analyse_et_visualisation_de_donn%C3%A9es.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Le but principal de ce notebook est de maîtriser <b>Spark SQL</b> et de pouvoir défnir des fonctions de fenêtre Spark et des fonctions Python enregistrées dans le registre Spark afin d'être utilisées dans les requêtes SQL.\n",
        "\n",
        "Le même jeu de données du précédent TP sera utilisé avec le même environnement d'exécution.\n"
      ],
      "metadata": {
        "id": "028oQK6oZSxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialisation de l'environnement d'exécution"
      ],
      "metadata": {
        "id": "9z7VzgdtcHT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation du JDK"
      ],
      "metadata": {
        "id": "DDl4tHX3G3up"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VG2QZ2-puh60"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Téléchargement de l'archive du framework Apache Spark"
      ],
      "metadata": {
        "id": "IZEGyLLqHF1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "8qPBj1B8tsrD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraction de l'archive dans le dossier courant <mark>/content</mark>"
      ],
      "metadata": {
        "id": "DCHQaWogHUe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the file\n",
        "!tar xf spark-3.3.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "seF5MAult-9S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation des modules Python <b>pyspark</b> et <b>findspark</b>"
      ],
      "metadata": {
        "id": "60GBDMzzHoFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "g5LvxUKjsBsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6dcbb1b-b5db-43da-c7f3-ba25452292f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test de l'installation de pyspark"
      ],
      "metadata": {
        "id": "aHs729CucFMv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y8u-WoOQZRcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18c6a00-5110-412a-a70f-bd6b658c97cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-3.3.1-bin-hadoop3/python/pyspark\n",
            "/content/spark-3.3.1-bin-hadoop3/python/pyspark/python/pyspark\n",
            "/content/spark-3.3.1-bin-hadoop3/bin/pyspark\n"
          ]
        }
      ],
      "source": [
        "!find /content -name \"pyspark\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Création des variables d'environnement <mark>SPARK_HOME</mark> et <mark>JAVA_HOME</mark> pour situer respectivement les emplacements d'installation de Spark et Java "
      ],
      "metadata": {
        "id": "-eppwCZiIdsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] =  \"/content/spark-3.3.1-bin-hadoop3\" \n",
        "os.environ[\"JAVA_HOME\"] =\"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "CSDjm_pZbWOW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importation des bibliothèques Spark SQL"
      ],
      "metadata": {
        "id": "HYkQctwUI4tE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "541d3a3c-4a20-41b6-9776-262e429052c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985b2e68-15e2-4b5d-87b0-07e3c0cee792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "findspark.init() initialise les variables d'environnement pour spark\n"
          ]
        }
      ],
      "source": [
        "import findspark \n",
        "print(\"findspark.init() initialise les variables d'environnement pour spark\") \n",
        "findspark.init() \n",
        "\n",
        "# Pyspark session objects\n",
        "from pyspark.sql import SparkSession \n",
        "# Pyspark session configuration\n",
        "from pyspark import SparkConf  \n",
        "\n",
        "# Pyspark functions\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql import * \n",
        "\n",
        "# Pyspark SQL data types\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyse et visualisation de données"
      ],
      "metadata": {
        "id": "WW1teUM7nB3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Définition de fonctions pour l'environnement PySpark"
      ],
      "metadata": {
        "id": "DIpOBtsGKRtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction <mark>demarrer_spark</mark> permet d'initialiser une session <i>client</i> avec Spark"
      ],
      "metadata": {
        "id": "-yq3xQUUKcVF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d1df52db-34b4-4298-b92d-f6df093f5715"
      },
      "outputs": [],
      "source": [
        "def demarrer_spark():\n",
        "  local = \"local[*]\"\n",
        "  appName = \"TP3\"\n",
        "  configLocale = SparkConf().setAppName(appName).setMaster(local).\\\n",
        "  set(\"spark.executor.memory\", \"100G\").\\\n",
        "  set(\"spark.driver.memory\",\"50G\").\\\n",
        "  set(\"spark.sql.catalogImplementation\",\"in-memory\").\\\n",
        "  set(\"spark.driver.maxResultSize\", \"10G\")\n",
        "  \n",
        "  spark = SparkSession.builder.config(conf = configLocale).getOrCreate()\n",
        "  sc = spark.sparkContext\n",
        "  sc.setLogLevel(\"ERROR\")\n",
        "  \n",
        "  # spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
        "  # On ajuste l'environnement d'exécution des requêtes à la taille du cluster (4 coeurs)\n",
        "  # spark.conf.set(\"spark.sql.shuffle.partitions\",\"200\")    \n",
        "\n",
        "  print(\"session démarrée, son id est \", sc.applicationId)\n",
        "  return spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Démarrage de la session"
      ],
      "metadata": {
        "id": "HvTT3nC6PDrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = demarrer_spark()"
      ],
      "metadata": {
        "id": "O_fkLyeXPPoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec6e62f3-4fed-4019-b6d9-1e000edead47"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "session démarrée, son id est  local-1675193987471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En vue de simplifier l'exécution des requêtes SQL, nous définissons la commande magique &#128526; <b><font color=\"blue\">%%sql</font></b> pour exécuter les requêtes plus facilement"
      ],
      "metadata": {
        "id": "N9WxdaTsrC0X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "202c9472-76de-4c92-98c1-6d55216aec7c"
      },
      "outputs": [],
      "source": [
        "from IPython.core.magic import (register_line_magic, register_cell_magic, register_line_cell_magic)\n",
        "import gc\n",
        "\n",
        "def removeComments(query):\n",
        "  result = \"\"\n",
        "  for line in query.split('\\n'):\n",
        "    if not(line.strip().startswith(\"--\")):\n",
        "      result += line + \"\\n\"\n",
        "  return result\n",
        "\n",
        "@register_line_cell_magic\n",
        "def sql(line, cell=None):\n",
        "    \"To run a sql query. Use:  %%sql\"\n",
        "    val = cell if cell is not None else line\n",
        "    tabRequetes = removeComments(val).split(\";\")\n",
        "    resultat = None\n",
        "    est_une_requete = False\n",
        "    for r in tabRequetes:\n",
        "        r = r.strip()\n",
        "        if len(r) > 2:\n",
        "          resultat = spark.sql(r)\n",
        "          est_une_requete = r.lower().startswith('select') or r.lower().startswith('with')  \n",
        "    if(est_une_requete):\n",
        "      # Explain the execution plan\n",
        "      #resultat.explain()\n",
        "      # Display the result\n",
        "      return display(resultat)\n",
        "    else:\n",
        "      return print('ok')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aed3e5e-d234-403b-a868-5a4a646a4955"
      },
      "source": [
        "De même, nous redéfinissons la fonction <b>display</b> pour un meilleur affichage des données manipulées."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5548ea24-d1cd-44fa-8137-34222b6e847a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0635d053-e933-44c3-c5b6-92dc54a39287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "display redéfini\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def display(df, n=10):\n",
        "  pd.set_option('max_columns', None)\n",
        "  pd.set_option('max_colwidth', None)\n",
        "  pdf = df.limit(n).toPandas()\n",
        "  # Free memory\n",
        "  df.unpersist()\n",
        "  # Force Spark to free memory\n",
        "  spark.catalog.clearCache()\n",
        "  # and Python too\n",
        "  gc.collect(2)\n",
        "  return pdf\n",
        "\n",
        "print(\"display redéfini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Définition de fonctions de visualisation"
      ],
      "metadata": {
        "id": "_XfT0cI5k91x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction d'exécution de requête SQL et conversion du résultat (un Dataframe Spark) en Dataframe Pandas"
      ],
      "metadata": {
        "id": "cQJDEuJDlGzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "def getPandasDataFrame(sqlQuery):\n",
        "  # Execute SQL Query with PySpark\n",
        "  dfSpark = spark.sql(sqlQuery)\n",
        "  # Convert Spark dataframe to Pandas dataframe\n",
        "  pdf = dfSpark.toPandas()\n",
        "  # Force Spark to free memory\n",
        "  dfSpark.unpersist()\n",
        "  spark.catalog.clearCache()\n",
        "  # and Python too\n",
        "  gc.collect(2)\n",
        "  # Return the Pandas Dataframe\n",
        "  return pdf"
      ],
      "metadata": {
        "id": "EaJqai9wTyqH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonctions de visualisation"
      ],
      "metadata": {
        "id": "mUM1YPcUt5-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import plotly.tools as pt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def drawLine(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the line chart\n",
        "  fig = px.line(pdf, x=pdf.columns[0], y=pdf.columns[1])\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawBar(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the bar chart\n",
        "  fig = px.bar(pdf, x=pdf.columns[0], y=pdf.columns[1])\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawHistogram(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the histogram chart\n",
        "  fig = px.histogram(pdf, x=pdf.columns[0], y=pdf.columns[1])\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawHeatmap(sql, scale=lambda x: x):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  if len(pdf.columns) != 3 and not (pdf[pdf.columns[2]].dtype == np.float64 or pdf[pdf.columns[2]].dtype == np.int64):\n",
        "    raise Exception(\"Sorry, no numbers below zero\")\n",
        "  source = pdf[pdf.columns[0]].tolist()\n",
        "  target = pdf[pdf.columns[1]].tolist()\n",
        "  value = pdf[pdf.columns[2]].tolist()\n",
        "  # plotting the figure\n",
        "  fig = go.Figure(data = go.Heatmap(x = source, y = target, z = [scale(x) for x in value])) \n",
        "  fig.show()\n",
        "\n",
        "def drawPie(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the pie chart\n",
        "  fig = px.pie(pdf, names=pdf.columns[0], values=pdf.columns[1])\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawStackedBar(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the stacked bar chart\n",
        "  fig = px.bar(df, x=pdf.columns[0], y=pdf.columns[2], color=pdf.columns[1], hover_data=pdf.columns[1], barmode = 'stack')\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawSankey(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  \n",
        "  labels = []\n",
        "  x = set(pdf[pdf.columns[0]].tolist())\n",
        "  dicX = {}\n",
        "  i = 0\n",
        "  for e in x:\n",
        "    dicX[e] = i\n",
        "    labels.append(e)\n",
        "    i += 1\n",
        "    \n",
        "  y = set(pdf[pdf.columns[1]].tolist())\n",
        "  dicY = {}\n",
        "  # i = len(labels)\n",
        "  for e in y:\n",
        "    if(e in dicX):\n",
        "      dicY[e] = dicX[e]\n",
        "    else:\n",
        "      dicY[e] = i\n",
        "      i += 1\n",
        "    labels.append(e)\n",
        "\n",
        "  fig = go.Figure(data=[go.Sankey(\n",
        "    node = dict(\n",
        "      thickness = 5,\n",
        "      line = dict(color = \"green\", width = 0.1),\n",
        "      label = labels,\n",
        "      color = \"blue\"\n",
        "    ),\n",
        "    link = dict(\n",
        "      # indices correspond to labels\n",
        "      source = [dicX[e] for e in pdf[pdf.columns[0]].tolist()],\n",
        "      target = [dicY[e] for e in pdf[pdf.columns[1]].tolist()],\n",
        "      value = pdf[pdf.columns[2]].tolist()\n",
        "  ))])\n",
        "\n",
        "  # showing the plot\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "c2qAiqSyt5eh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Récupération du jeu de données"
      ],
      "metadata": {
        "id": "qtZl2WZvjtqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Téléchargement du jeu"
      ],
      "metadata": {
        "id": "1KdpjxvkjxQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store?select=2019-Nov.csv"
      ],
      "metadata": {
        "id": "FcbFT7RSUe8Z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o ecommerce-behavior-data-from-multi-category-store.zip 'https://drive.google.com/u/0/uc?id=1CVhmxsU3GY0FYGS1uP3m_tGbyGjEfuQc&export=download&confirm=t'\n",
        "#!curl -L -o ecommerce-behavior-data-from-multi-category-store.zip 'https://storage.googleapis.com/kaggle-data-sets/411512/835452/compressed/2019-Nov.csv.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230128%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230128T111731Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2afcc1a011c86d89fe64d1c12bc1432f703c525f12c74f43fad4f455a5183c74589932fbabb73bce85de91427906abecec18c6929a894fd0ca8657683b665379deea648ef51f6bb4c114125998ee24b7fdd2b630cdc327e142d0f8130f2f5e9306d45293940e87b2c05aa32151f52ab4a85638d5920e6de0fbf13b8daaffd7fbeb21009fc42c8baf268a399a1419b0bf0c9a5a5150732d0d10d4a1b90c7b516d60a01ffb2dc3b42c9266f3acdecf42b791a074f379ec89295af92a337d89af4f092e6a74db6b74f75305604e9593e265dafdf6e25dbe9b9160840864260541f1a188473fa9c59514fd0d4136cd04066084275d95e238525b94333cac9a6b6ceb'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OxKIr63RSNa",
        "outputId": "10ca21af-2a03-4848-f6cc-e9019485f425"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 4393M  100 4393M    0     0  70.8M      0  0:01:02  0:01:02 --:--:-- 81.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraction des données"
      ],
      "metadata": {
        "id": "P2uCCY-bkC83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o ecommerce-behavior-data-from-multi-category-store.zip\n",
        "!ls ."
      ],
      "metadata": {
        "id": "QMZvksQ4gAMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568e013d-aed0-4c0d-ded1-368275eefafd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ecommerce-behavior-data-from-multi-category-store.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of ecommerce-behavior-data-from-multi-category-store.zip or\n",
            "        ecommerce-behavior-data-from-multi-category-store.zip.zip, and cannot find ecommerce-behavior-data-from-multi-category-store.zip.ZIP, period.\n",
            "'ecommerce-behavior-data-from-multi-category-store?select=2019-Nov.csv'\n",
            " ecommerce-behavior-data-from-multi-category-store.zip\n",
            " sample_data\n",
            " spark-3.3.1-bin-hadoop3\n",
            " spark-3.3.1-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aperçu de format des données"
      ],
      "metadata": {
        "id": "hUYJ9R3aj-DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 2019-Nov.csv"
      ],
      "metadata": {
        "id": "fYLvhFctgGl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c84f669-5086-4bc7-f763-4b78101d879b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head: cannot open '2019-Nov.csv' for reading: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chargement du jeu de données dans Spark"
      ],
      "metadata": {
        "id": "sEOqnQdykGvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"2019-Nov.csv\", header=True, sep=',')\n",
        "display(df)"
      ],
      "metadata": {
        "id": "NkF2bSVljVTm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "2e992717-6282-4d71-d8f8-0d48a8059aa3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-bdc8326ae760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2019-Nov.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/2019-Nov.csv"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Affichage du nombre d'enregistrements du jeu de données"
      ],
      "metadata": {
        "id": "T5EBOlXXkK6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "OMCijnajlg_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Affichage du schéma du Dataframe Spark"
      ],
      "metadata": {
        "id": "UQ7A5JrnkQZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "jPMar0nxsEm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casting de certaines colonnes aux types de données attendus"
      ],
      "metadata": {
        "id": "xzRTNlFlkXW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"event_time\",df.event_time.cast(TimestampType()))\n",
        "df = df.withColumn(\"product_id\",df.product_id.cast(IntegerType()))\n",
        "df = df.withColumn(\"category_id\",df.category_id.cast(IntegerType()))\n",
        "df = df.withColumn(\"price\",df.price.cast(DoubleType()))\n",
        "df = df.withColumn(\"user_id\",df.user_id.cast(IntegerType()))"
      ],
      "metadata": {
        "id": "Nz-RYB5HsK86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Affichage du nouveau schéma du Dataframe Spark"
      ],
      "metadata": {
        "id": "SsRzcWl6kdtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "GATNQ7gusulc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matérialisation du dataframe comme une vue SQL avec la vue <mark>purchases</mark> qui pointe sur lui"
      ],
      "metadata": {
        "id": "gyHcPe8JkjmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView('events')"
      ],
      "metadata": {
        "id": "-tfrmaQ1lxmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activité 1"
      ],
      "metadata": {
        "id": "HBTAvjIVmjZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test de notre première requête SQL. <b>Que fait-elle ?</b>"
      ],
      "metadata": {
        "id": "CUtiK71Qy6PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SELECT brand, event_type, COUNT(*) AS nb FROM events GROUP BY brand, event_type"
      ],
      "metadata": {
        "id": "xWuIdvcGo-Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que fait cette deuxième requête ?"
      ],
      "metadata": {
        "id": "76RUKgEczA7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drawLine(\"SELECT brand, COUNT(*) AS nb FROM events GROUP BY brand\")"
      ],
      "metadata": {
        "id": "8_IzgUBTpCyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici la liste des types d'évènement possibles : <i>purchase, view, cart</i> ..."
      ],
      "metadata": {
        "id": "b5cX-J6kzNyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SELECT DISTINCT event_type FROM events"
      ],
      "metadata": {
        "id": "LSRVPPg1t3FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et leur répartition dans le jeu de données"
      ],
      "metadata": {
        "id": "h8AghSiNzf3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drawPie(\"SELECT event_type, COUNT(*) FROM events GROUP BY event_type\")"
      ],
      "metadata": {
        "id": "sBMrwFQDuAh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment comprenez-vous cette requête et sa visualisation ?"
      ],
      "metadata": {
        "id": "DTg8ttYpzpu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql = \" SELECT brand, event_type, COUNT(*) AS nb FROM events GROUP BY brand, event_type\"\n",
        "drawHeatmap(sql, scale=math.log)"
      ],
      "metadata": {
        "id": "SH1jfwciuO2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que fait cette dernière requête ?\n",
        "Comment comprenez-vous sa graphique ?"
      ],
      "metadata": {
        "id": "uoeJ5yIKz4kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql = \"SELECT category_code, date_format(event_time, 'L'), SUM(price) AS amount FROM events WHERE event_type='purchase' GROUP BY category_code, date_format(event_time, 'L') HAVING category_code LIKE 'electronics.%'\"\n",
        "drawSankey(sql)"
      ],
      "metadata": {
        "id": "JCZfROUShnRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = \"SELECT event_type as A, category_code as B, COUNT(*) FROM events WHERE event_type='cart' GROUP BY event_type, category_code \\\n",
        " UNION SELECT category_code as A, event_type as B, COUNT(*) FROM events WHERE event_type='purchase' GROUP BY event_type, category_code\"\n",
        "drawSankey(sql)"
      ],
      "metadata": {
        "id": "pCmSS1SC33Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.clearCache()\n",
        "gc.collect(2)"
      ],
      "metadata": {
        "id": "jHqtpWDVvXZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activité 2\n",
        "Nous continuons à travailler avec la même vue <mark>events</mark>. Le but est d’écrire vos propres requêtes <u>et de réflexir sur leurs meilleures présentations</u>.\n",
        "<ol>\n",
        "<li>Quel est le chiffre d'affaire réalisé selon les jours de la semaine ?</li>\n",
        "<li>Donner l'évolution du nombre d'achats de produits selon les jours du mois.</li>\n",
        "<li>Donner le top 3 des catégories de produits et leurs chiffres d'affaires par type d'évènement.</li>\n",
        "<li>Donner le chiffre d'affaire gagné sur chaque marque (brand) selon le jour de la semaine.</li>\n",
        "<li>Donner le top 3 des marques pour catégorie de produit.</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "khg1XVe30q7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Quel est le chiffre d'affaire réalisé selon les jours de la semaine ?\n"
      ],
      "metadata": {
        "id": "rJ9jJVgT0xqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Donner l'évolution du nombre d'achats de produits selon les jours du mois.\n"
      ],
      "metadata": {
        "id": "zuBvZlsU6572"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Donner le top 3 des catégories de produits par type d'évènement.\n"
      ],
      "metadata": {
        "id": "ea_jf7vK65bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Donner le chiffre d'affaire gagné sur chaque marque.\n"
      ],
      "metadata": {
        "id": "qiGlabEh63kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Donner le top 3 des marques pour catégorie de produit.\n"
      ],
      "metadata": {
        "id": "KZEbKVw764kl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}