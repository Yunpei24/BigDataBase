{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yunpei24/BigDataBase/blob/main/ProjetBigData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z7VzgdtcHT7"
      },
      "source": [
        "# Initialisation de l'environnement d'exécution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDl4tHX3G3up"
      },
      "source": [
        "Installation du JDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VG2QZ2-puh60"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZEGyLLqHF1T"
      },
      "source": [
        "Téléchargement de l'archive du framework Apache Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8qPBj1B8tsrD"
      },
      "outputs": [],
      "source": [
        "# Download Spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCHQaWogHUe-"
      },
      "source": [
        "Extraction de l'archive dans le dossier courant <mark>/content</mark>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "seF5MAult-9S"
      },
      "outputs": [],
      "source": [
        "# Unzip the file\n",
        "!tar xf spark-3.3.1-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60GBDMzzHoFr"
      },
      "source": [
        "Installation des modules Python <b>pyspark</b> et <b>findspark</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "g5LvxUKjsBsR"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHs729CucFMv"
      },
      "source": [
        "Test de l'installation de pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8u-WoOQZRcV",
        "outputId": "0ac181ef-3ca5-4a35-f0ba-c176e6086aef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-3.3.1-bin-hadoop3/python/pyspark\n",
            "/content/spark-3.3.1-bin-hadoop3/python/pyspark/python/pyspark\n",
            "/content/spark-3.3.1-bin-hadoop3/bin/pyspark\n"
          ]
        }
      ],
      "source": [
        "!find /content -name \"pyspark\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eppwCZiIdsg"
      },
      "source": [
        "Création des variables d'environnement <mark>SPARK_HOME</mark> et <mark>JAVA_HOME</mark> pour situer respectivement les emplacements d'installation de Spark et Java "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CSDjm_pZbWOW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] =  \"/content/spark-3.3.1-bin-hadoop3\" \n",
        "os.environ[\"JAVA_HOME\"] =\"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYkQctwUI4tE"
      },
      "source": [
        "Importation des bibliothèques Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "541d3a3c-4a20-41b6-9776-262e429052c5",
        "outputId": "16811e76-65f3-44f3-f63c-92171bd045f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "findspark.init() initialise les variables d'environnement pour spark\n"
          ]
        }
      ],
      "source": [
        "import findspark \n",
        "print(\"findspark.init() initialise les variables d'environnement pour spark\") \n",
        "findspark.init() \n",
        "\n",
        "# Pyspark session objects\n",
        "from pyspark.sql import SparkSession \n",
        "# Pyspark session configuration\n",
        "from pyspark import SparkConf  \n",
        "\n",
        "# Pyspark functions\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql import * \n",
        "\n",
        "# Pyspark SQL data types\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW1teUM7nB3b"
      },
      "source": [
        "# Analyse et visualisation de données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIpOBtsGKRtu"
      },
      "source": [
        "## Définition de fonctions pour l'environnement PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yq3xQUUKcVF"
      },
      "source": [
        "La fonction <mark>demarrer_spark</mark> permet d'initialiser une session <i>client</i> avec Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "d1df52db-34b4-4298-b92d-f6df093f5715"
      },
      "outputs": [],
      "source": [
        "def demarrer_spark():\n",
        "  local = \"local[*]\"\n",
        "  appName = \"TP3\"\n",
        "  configLocale = SparkConf().setAppName(appName).setMaster(local).\\\n",
        "  set(\"spark.executor.memory\", \"100G\").\\\n",
        "  set(\"spark.driver.memory\",\"50G\").\\\n",
        "  set(\"spark.sql.catalogImplementation\",\"in-memory\").\\\n",
        "  set(\"spark.driver.maxResultSize\", \"10G\")\n",
        "  \n",
        "  spark = SparkSession.builder.config(conf = configLocale).getOrCreate()\n",
        "  sc = spark.sparkContext\n",
        "  sc.setLogLevel(\"ERROR\")\n",
        "  \n",
        "  # spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
        "  # On ajuste l'environnement d'exécution des requêtes à la taille du cluster (4 coeurs)\n",
        "  # spark.conf.set(\"spark.sql.shuffle.partitions\",\"200\")    \n",
        "\n",
        "  print(\"session démarrée, son id est \", sc.applicationId)\n",
        "  return spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvTT3nC6PDrw"
      },
      "source": [
        "Démarrage de la session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "O_fkLyeXPPoY",
        "outputId": "d36504f8-498a-44d8-8d54-12f24bdbfd46"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionRefusedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-d483d6cd1142>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemarrer_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-5ef81b3ed995>\u001b[0m in \u001b[0;36mdemarrer_spark\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mappName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"TP3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mconfigLocale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.executor.memory\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"100G\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.driver.memory\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"50G\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/pyspark/conf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;31m# JVM is created, so create self._jconf directly through JVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadDefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         answer = self._gateway_client.send_command(\n\u001b[0m\u001b[1;32m   1710\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
          ]
        }
      ],
      "source": [
        "spark = demarrer_spark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9WxdaTsrC0X"
      },
      "source": [
        "En vue de simplifier l'exécution des requêtes SQL, nous définissons la commande magique &#128526; <b><font color=\"blue\">%%sql</font></b> pour exécuter les requêtes plus facilement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "202c9472-76de-4c92-98c1-6d55216aec7c"
      },
      "outputs": [],
      "source": [
        "from IPython.core.magic import (register_line_magic, register_cell_magic, register_line_cell_magic)\n",
        "import gc\n",
        "\n",
        "def removeComments(query):\n",
        "  result = \"\"\n",
        "  for line in query.split('\\n'):\n",
        "    if not(line.strip().startswith(\"--\")):\n",
        "      result += line + \"\\n\"\n",
        "  return result\n",
        "\n",
        "@register_line_cell_magic\n",
        "def sql(line, cell=None):\n",
        "    \"To run a sql query. Use:  %%sql\"\n",
        "    val = cell if cell is not None else line\n",
        "    tabRequetes = removeComments(val).split(\";\")\n",
        "    resultat = None\n",
        "    est_une_requete = False\n",
        "    for r in tabRequetes:\n",
        "        r = r.strip()\n",
        "        if len(r) > 2:\n",
        "          resultat = spark.sql(r)\n",
        "          est_une_requete = r.lower().startswith('select') or r.lower().startswith('with')  \n",
        "    if(est_une_requete):\n",
        "      # Explain the execution plan\n",
        "      #resultat.explain()\n",
        "      # Display the result\n",
        "      return display(resultat)\n",
        "    else:\n",
        "      return print('ok')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aed3e5e-d234-403b-a868-5a4a646a4955"
      },
      "source": [
        "De même, nous redéfinissons la fonction <b>display</b> pour un meilleur affichage des données manipulées."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5548ea24-d1cd-44fa-8137-34222b6e847a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def display(df, n=10):\n",
        "  pd.set_option('max_columns', None)\n",
        "  pd.set_option('max_colwidth', None)\n",
        "  pdf = df.limit(n).toPandas()\n",
        "  # Free memory\n",
        "  df.unpersist()\n",
        "  # Force Spark to free memory\n",
        "  spark.catalog.clearCache()\n",
        "  # and Python too\n",
        "  gc.collect(2)\n",
        "  return pdf\n",
        "\n",
        "print(\"display redéfini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XfT0cI5k91x"
      },
      "source": [
        "## Définition de fonctions de visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQJDEuJDlGzO"
      },
      "source": [
        "Fonction d'exécution de requête SQL et conversion du résultat (un Dataframe Spark) en Dataframe Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaJqai9wTyqH"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "def getPandasDataFrame(sqlQuery):\n",
        "  # Execute SQL Query with PySpark\n",
        "  dfSpark = spark.sql(sqlQuery)\n",
        "  # Convert Spark dataframe to Pandas dataframe\n",
        "  pdf = dfSpark.toPandas()\n",
        "  # Force Spark to free memory\n",
        "  dfSpark.unpersist()\n",
        "  spark.catalog.clearCache()\n",
        "  # and Python too\n",
        "  gc.collect(2)\n",
        "  # Return the Pandas Dataframe\n",
        "  return pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUM1YPcUt5-9"
      },
      "source": [
        "Fonctions de visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2qAiqSyt5eh"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import plotly.tools as pt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def drawLine(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the line chart\n",
        "  fig = px.line(pdf, x=pdf.columns[0], y=pdf.columns[1])\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawBar(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the bar chart\n",
        "  fig = px.bar(pdf, x=pdf.columns[0], y=pdf.columns[1])\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawHistogram(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the histogram chart\n",
        "  fig = px.histogram(pdf, x=pdf.columns[0], y=pdf.columns[1])\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawHeatmap(sql, scale=lambda x: x):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  if len(pdf.columns) != 3 and not (pdf[pdf.columns[2]].dtype == np.float64 or pdf[pdf.columns[2]].dtype == np.int64):\n",
        "    raise Exception(\"Sorry, no numbers below zero\")\n",
        "  source = pdf[pdf.columns[0]].tolist()\n",
        "  target = pdf[pdf.columns[1]].tolist()\n",
        "  value = pdf[pdf.columns[2]].tolist()\n",
        "  # plotting the figure\n",
        "  fig = go.Figure(data = go.Heatmap(x = source, y = target, z = [scale(x) for x in value])) \n",
        "  fig.show()\n",
        "\n",
        "def drawPie(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the pie chart\n",
        "  fig = px.pie(pdf, names=pdf.columns[0], values=pdf.columns[1])\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawStackedBar(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the stacked bar chart\n",
        "  fig = px.bar(df, x=pdf.columns[0], y=pdf.columns[2], color=pdf.columns[1], hover_data=pdf.columns[1], barmode = 'stack')\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawSankey(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  \n",
        "  labels = []\n",
        "  x = set(pdf[pdf.columns[0]].tolist())\n",
        "  dicX = {}\n",
        "  i = 0\n",
        "  for e in x:\n",
        "    dicX[e] = i\n",
        "    labels.append(e)\n",
        "    i += 1\n",
        "    \n",
        "  y = set(pdf[pdf.columns[1]].tolist())\n",
        "  dicY = {}\n",
        "  # i = len(labels)\n",
        "  for e in y:\n",
        "    if(e in dicX):\n",
        "      dicY[e] = dicX[e]\n",
        "    else:\n",
        "      dicY[e] = i\n",
        "      i += 1\n",
        "    labels.append(e)\n",
        "\n",
        "  fig = go.Figure(data=[go.Sankey(\n",
        "    node = dict(\n",
        "      thickness = 5,\n",
        "      line = dict(color = \"green\", width = 0.1),\n",
        "      label = labels,\n",
        "      color = \"blue\"\n",
        "    ),\n",
        "    link = dict(\n",
        "      # indices correspond to labels\n",
        "      source = [dicX[e] for e in pdf[pdf.columns[0]].tolist()],\n",
        "      target = [dicY[e] for e in pdf[pdf.columns[1]].tolist()],\n",
        "      value = pdf[pdf.columns[2]].tolist()\n",
        "  ))])\n",
        "\n",
        "  # showing the plot\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtZl2WZvjtqy"
      },
      "source": [
        "## Récupération du jeu de données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KdpjxvkjxQM"
      },
      "source": [
        "Téléchargement du jeu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v95gxMsAJxIO"
      },
      "outputs": [],
      "source": [
        "!curl -L -o ecommerce-behavior-data-from-multi-category-store.zip 'https://drive.google.com/u/0/uc?id=1CVhmxsU3GY0FYGS1uP3m_tGbyGjEfuQc&export=download&confirm=t'\n",
        "#!curl -L -o ecommerce-behavior-data-from-multi-category-store.zip 'https://storage.googleapis.com/kaggle-data-sets/411512/835452/compressed/2019-Nov.csv.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230128%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230128T111731Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2afcc1a011c86d89fe64d1c12bc1432f703c525f12c74f43fad4f455a5183c74589932fbabb73bce85de91427906abecec18c6929a894fd0ca8657683b665379deea648ef51f6bb4c114125998ee24b7fdd2b630cdc327e142d0f8130f2f5e9306d45293940e87b2c05aa32151f52ab4a85638d5920e6de0fbf13b8daaffd7fbeb21009fc42c8baf268a399a1419b0bf0c9a5a5150732d0d10d4a1b90c7b516d60a01ffb2dc3b42c9266f3acdecf42b791a074f379ec89295af92a337d89af4f092e6a74db6b74f75305604e9593e265dafdf6e25dbe9b9160840864260541f1a188473fa9c59514fd0d4136cd04066084275d95e238525b94333cac9a6b6ceb'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2uCCY-bkC83"
      },
      "source": [
        "Extraction des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMZvksQ4gAMH"
      },
      "outputs": [],
      "source": [
        "!unzip -o ecommerce-behavior-data-from-multi-category-store.zip\n",
        "!ls ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUYJ9R3aj-DL"
      },
      "source": [
        "Aperçu de format des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEOqnQdykGvr"
      },
      "source": [
        "Chargement du jeu de données dans Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP8Sb2g_NwZj"
      },
      "outputs": [],
      "source": [
        "#!head -10 2019-Nov.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkF2bSVljVTm"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"2019-Nov.csv\", header=True, sep=',')\n",
        "#display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5EBOlXXkK6u"
      },
      "source": [
        "Affichage du nombre d'enregistrements du jeu de données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ7A5JrnkQZS"
      },
      "source": [
        "Affichage du schéma du Dataframe Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPMar0nxsEm8"
      },
      "outputs": [],
      "source": [
        "#df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzRTNlFlkXW9"
      },
      "source": [
        "Casting de certaines colonnes aux types de données attendus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz-RYB5HsK86"
      },
      "outputs": [],
      "source": [
        "df = df.withColumn(\"event_time\",df.event_time.cast(TimestampType()))\n",
        "df = df.withColumn(\"product_id\",df.product_id.cast(IntegerType()))\n",
        "df = df.withColumn(\"category_id\",df.category_id.cast(IntegerType()))\n",
        "df = df.withColumn(\"price\",df.price.cast(DoubleType()))\n",
        "df = df.withColumn(\"user_id\",df.user_id.cast(IntegerType()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsRzcWl6kdtC"
      },
      "source": [
        "Affichage du nouveau schéma du Dataframe Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GATNQ7gusulc"
      },
      "outputs": [],
      "source": [
        "#df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyHcPe8JkjmJ"
      },
      "source": [
        "Matérialisation du dataframe comme une vue SQL avec la vue <mark>purchases</mark> qui pointe sur lui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tfrmaQ1lxmP"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView('events')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irFoVU91EpbT"
      },
      "source": [
        "# 1. Le chiffre d'affaire réalisé selon les jours de la semaine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHKpXkglFu5y"
      },
      "outputs": [],
      "source": [
        "dayNames = ['', 'Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche']\n",
        "  \n",
        "def weekDayName(dayID):\n",
        "  global dayNames\n",
        "  val = int(dayID)\n",
        "  if(1 <= val <= 7):\n",
        "    return dayNames[val]\n",
        "  else:\n",
        "    return \"Unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Nsz1JT9Fyse"
      },
      "outputs": [],
      "source": [
        "spark.udf.register(\"weekDayName\", weekDayName, StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6-bJPN1EiA6"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT weekDayName(date_format(event_time, 'F')) AS JoursSemaine, SUM(price) AS ChiffreAffaire FROM events WHERE event_type= 'purchase' GROUP BY weekDayName(date_format(event_time, 'F'))\"\n",
        "drawHistogram(sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWrd2DH-2Xal"
      },
      "source": [
        "# 2. Donnons l'évolution du nombre d'achats de produits selon les jours du mois."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extractDay(day):\n",
        "  day1 = str(day).strip().split(\" \")\n",
        "  if len(day1) == 2:\n",
        "    Date, Heure = day1\n",
        "    return Date"
      ],
      "metadata": {
        "id": "ltp2kuEo_YQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.udf.register(\"extractDay\", extractDay, StringType())"
      ],
      "metadata": {
        "id": "pVVCMM_p_10P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oHlvWW12s2F"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT extractDay(event_time) AS JoursMois, COUNT(event_type) FROM events WHERE event_type='purchase' GROUP BY extractDay(event_time)\"\n",
        "drawLine(sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Donnons le top 3 des catégories de produits et leurs chiffres d'affaires par type d'évènement."
      ],
      "metadata": {
        "id": "kzgUb0DXCU5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "CREATE OR REPLACE TEMP VIEW TopProd AS SELECT category_code, event_type, SUM(price) AS chiffreDaffaire FROM events WHERE category_code != 'None' GROUP BY category_code, event_type"
      ],
      "metadata": {
        "id": "vA3PIU8VKZ68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "CREATE OR REPLACE TEMP VIEW TopProd2 AS SELECT category_code, event_type, chiffreDaffaire, RANK() over (PARTITION BY event_type ORDER BY chiffreDaffaire DESC) AS rang FROM TopProd"
      ],
      "metadata": {
        "id": "rvFFWYlJLMZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Donnons le top 3 des catégories de produits et leurs chiffres d'affaires par type d'évènement\n",
        "sql = \"SELECT event_type, category_code, chiffreDaffaire FROM TopProd2 WHERE rang <= 3\"\n",
        "drawHeatmap(sql, scale=math.log)"
      ],
      "metadata": {
        "id": "MGcihL1BMK4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Donnons le chiffre d'affaire gagné sur chaque marque (brand) selon le jour de la semaine."
      ],
      "metadata": {
        "id": "6DqJjSoV0qLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql = \"SELECT brand AS Marque, weekDayName(date_format(event_time, 'F')) AS JoursSemaine, SUM(price) AS chiffreDaffaire FROM events GROUP BY weekDayName(date_format(event_time, 'F')), brand\"\n",
        "drawHeatmap(sql, scale=math.log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "UvN0p2Jt0pJq",
        "outputId": "ce724847-0282-4433-c3fd-ec39aa85b360"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionRefusedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-8608eeaab2cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SELECT brand AS Marque, weekDayName(date_format(event_time, 'F')) AS JoursSemaine, SUM(price) AS chiffreDaffaire FROM events GROUP BY weekDayName(date_format(event_time, 'F')), brand\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrawHeatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-f452e47b04b8>\u001b[0m in \u001b[0;36mdrawHeatmap\u001b[0;34m(sql, scale)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdrawHeatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;31m# Getting Pandas Dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPandasDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sorry, no numbers below zero\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-f2c81c8e4397>\u001b[0m in \u001b[0;36mgetPandasDataFrame\u001b[0;34m(sqlQuery)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetPandasDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Execute SQL Query with PySpark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mdfSpark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;31m# Convert Spark dataframe to Pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfSpark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAyiNumIAuGt1q8AkZ54sb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}