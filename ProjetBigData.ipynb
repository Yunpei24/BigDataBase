{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yunpei24/BigDataBase/blob/main/ProjetBigData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z7VzgdtcHT7"
      },
      "source": [
        "# Initialisation de l'environnement d'exécution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDl4tHX3G3up"
      },
      "source": [
        "Installation du JDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VG2QZ2-puh60"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZEGyLLqHF1T"
      },
      "source": [
        "Téléchargement de l'archive du framework Apache Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8qPBj1B8tsrD"
      },
      "outputs": [],
      "source": [
        "# Download Spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCHQaWogHUe-"
      },
      "source": [
        "Extraction de l'archive dans le dossier courant <mark>/content</mark>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "seF5MAult-9S"
      },
      "outputs": [],
      "source": [
        "# Unzip the file\n",
        "!tar xf spark-3.3.1-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60GBDMzzHoFr"
      },
      "source": [
        "Installation des modules Python <b>pyspark</b> et <b>findspark</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g5LvxUKjsBsR",
        "outputId": "8b131832-1799-4ec4-a7cd-bb96972c48d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHs729CucFMv"
      },
      "source": [
        "Test de l'installation de pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8u-WoOQZRcV",
        "outputId": "6f96c839-1fbe-4002-fd08-dded15d02f8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-3.3.1-bin-hadoop3/python/pyspark\n",
            "/content/spark-3.3.1-bin-hadoop3/python/pyspark/python/pyspark\n",
            "/content/spark-3.3.1-bin-hadoop3/bin/pyspark\n"
          ]
        }
      ],
      "source": [
        "!find /content -name \"pyspark\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eppwCZiIdsg"
      },
      "source": [
        "Création des variables d'environnement <mark>SPARK_HOME</mark> et <mark>JAVA_HOME</mark> pour situer respectivement les emplacements d'installation de Spark et Java "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CSDjm_pZbWOW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] =  \"/content/spark-3.3.1-bin-hadoop3\" \n",
        "os.environ[\"JAVA_HOME\"] =\"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYkQctwUI4tE"
      },
      "source": [
        "Importation des bibliothèques Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "541d3a3c-4a20-41b6-9776-262e429052c5",
        "outputId": "6c281956-a8a6-450a-cdf3-2cc733a9edeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "findspark.init() initialise les variables d'environnement pour spark\n"
          ]
        }
      ],
      "source": [
        "import findspark \n",
        "print(\"findspark.init() initialise les variables d'environnement pour spark\") \n",
        "findspark.init() \n",
        "\n",
        "# Pyspark session objects\n",
        "from pyspark.sql import SparkSession \n",
        "# Pyspark session configuration\n",
        "from pyspark import SparkConf  \n",
        "\n",
        "# Pyspark functions\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql import * \n",
        "\n",
        "# Pyspark SQL data types\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW1teUM7nB3b"
      },
      "source": [
        "# Analyse et visualisation de données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIpOBtsGKRtu"
      },
      "source": [
        "## Définition de fonctions pour l'environnement PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yq3xQUUKcVF"
      },
      "source": [
        "La fonction <mark>demarrer_spark</mark> permet d'initialiser une session <i>client</i> avec Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d1df52db-34b4-4298-b92d-f6df093f5715"
      },
      "outputs": [],
      "source": [
        "def demarrer_spark():\n",
        "  local = \"local[*]\"\n",
        "  appName = \"TP3\"\n",
        "  configLocale = SparkConf().setAppName(appName).setMaster(local).\\\n",
        "  set(\"spark.executor.memory\", \"100G\").\\\n",
        "  set(\"spark.driver.memory\",\"50G\").\\\n",
        "  set(\"spark.sql.catalogImplementation\",\"in-memory\").\\\n",
        "  set(\"spark.driver.maxResultSize\", \"10G\")\n",
        "  \n",
        "  spark = SparkSession.builder.config(conf = configLocale).getOrCreate()\n",
        "  sc = spark.sparkContext\n",
        "  sc.setLogLevel(\"ERROR\")\n",
        "  \n",
        "  # spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
        "  # On ajuste l'environnement d'exécution des requêtes à la taille du cluster (4 coeurs)\n",
        "  # spark.conf.set(\"spark.sql.shuffle.partitions\",\"200\")    \n",
        "\n",
        "  print(\"session démarrée, son id est \", sc.applicationId)\n",
        "  return spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvTT3nC6PDrw"
      },
      "source": [
        "Démarrage de la session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_fkLyeXPPoY",
        "outputId": "6bb04cba-68c6-4b88-ed91-2d022bf42c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "session démarrée, son id est  local-1675343182017\n"
          ]
        }
      ],
      "source": [
        "spark = demarrer_spark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9WxdaTsrC0X"
      },
      "source": [
        "En vue de simplifier l'exécution des requêtes SQL, nous définissons la commande magique &#128526; <b><font color=\"blue\">%%sql</font></b> pour exécuter les requêtes plus facilement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "202c9472-76de-4c92-98c1-6d55216aec7c"
      },
      "outputs": [],
      "source": [
        "from IPython.core.magic import (register_line_magic, register_cell_magic, register_line_cell_magic)\n",
        "import gc\n",
        "\n",
        "def removeComments(query):\n",
        "  result = \"\"\n",
        "  for line in query.split('\\n'):\n",
        "    if not(line.strip().startswith(\"--\")):\n",
        "      result += line + \"\\n\"\n",
        "  return result\n",
        "\n",
        "@register_line_cell_magic\n",
        "def sql(line, cell=None):\n",
        "    \"To run a sql query. Use:  %%sql\"\n",
        "    val = cell if cell is not None else line\n",
        "    tabRequetes = removeComments(val).split(\";\")\n",
        "    resultat = None\n",
        "    est_une_requete = False\n",
        "    for r in tabRequetes:\n",
        "        r = r.strip()\n",
        "        if len(r) > 2:\n",
        "          resultat = spark.sql(r)\n",
        "          est_une_requete = r.lower().startswith('select') or r.lower().startswith('with')  \n",
        "    if(est_une_requete):\n",
        "      # Explain the execution plan\n",
        "      #resultat.explain()\n",
        "      # Display the result\n",
        "      return display(resultat)\n",
        "    else:\n",
        "      return print('ok')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aed3e5e-d234-403b-a868-5a4a646a4955"
      },
      "source": [
        "De même, nous redéfinissons la fonction <b>display</b> pour un meilleur affichage des données manipulées."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5548ea24-d1cd-44fa-8137-34222b6e847a",
        "outputId": "89147641-9c90-46e2-ea27-4a8069e202cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "display redéfini\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def display(df, n=10):\n",
        "  pd.set_option('max_columns', None)\n",
        "  pd.set_option('max_colwidth', None)\n",
        "  pdf = df.limit(n).toPandas()\n",
        "  # Free memory\n",
        "  df.unpersist()\n",
        "  # Force Spark to free memory\n",
        "  spark.catalog.clearCache()\n",
        "  # and Python too\n",
        "  gc.collect(2)\n",
        "  return pdf\n",
        "\n",
        "print(\"display redéfini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XfT0cI5k91x"
      },
      "source": [
        "## Définition de fonctions de visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQJDEuJDlGzO"
      },
      "source": [
        "Fonction d'exécution de requête SQL et conversion du résultat (un Dataframe Spark) en Dataframe Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EaJqai9wTyqH"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "def getPandasDataFrame(sqlQuery):\n",
        "  # Execute SQL Query with PySpark\n",
        "  dfSpark = spark.sql(sqlQuery)\n",
        "  # Convert Spark dataframe to Pandas dataframe\n",
        "  pdf = dfSpark.toPandas()\n",
        "  # Force Spark to free memory\n",
        "  dfSpark.unpersist()\n",
        "  spark.catalog.clearCache()\n",
        "  # and Python too\n",
        "  gc.collect(2)\n",
        "  # Return the Pandas Dataframe\n",
        "  return pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUM1YPcUt5-9"
      },
      "source": [
        "Fonctions de visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c2qAiqSyt5eh"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import plotly.tools as pt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def drawLine(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the line chart\n",
        "  fig = px.line(pdf, x=pdf.columns[0], y=pdf.columns[1])\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawBar(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the bar chart\n",
        "  fig = px.bar(pdf, x=pdf.columns[0], y=pdf.columns[1])\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawHistogram(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the histogram chart\n",
        "  fig = px.histogram(pdf, x=pdf.columns[0], y=pdf.columns[1])\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawHeatmap(sql, scale=lambda x: x):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  if len(pdf.columns) != 3 and not (pdf[pdf.columns[2]].dtype == np.float64 or pdf[pdf.columns[2]].dtype == np.int64):\n",
        "    raise Exception(\"Sorry, no numbers below zero\")\n",
        "  source = pdf[pdf.columns[0]].tolist()\n",
        "  target = pdf[pdf.columns[1]].tolist()\n",
        "  value = pdf[pdf.columns[2]].tolist()\n",
        "  # plotting the figure\n",
        "  fig = go.Figure(data = go.Heatmap(x = source, y = target, z = [scale(x) for x in value])) \n",
        "  fig.show()\n",
        "\n",
        "def drawPie(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the pie chart\n",
        "  fig = px.pie(pdf, names=pdf.columns[0], values=pdf.columns[1])\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawStackedBar(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  # plotting the stacked bar chart\n",
        "  fig = px.bar(df, x=pdf.columns[0], y=pdf.columns[2], color=pdf.columns[1], hover_data=pdf.columns[1], barmode = 'stack')\n",
        "  # showing the plot\n",
        "  fig.show()\n",
        "\n",
        "def drawSankey(sql):\n",
        "  # Getting Pandas Dataframe\n",
        "  pdf = getPandasDataFrame(sql)\n",
        "  \n",
        "  labels = []\n",
        "  x = set(pdf[pdf.columns[0]].tolist())\n",
        "  dicX = {}\n",
        "  i = 0\n",
        "  for e in x:\n",
        "    dicX[e] = i\n",
        "    labels.append(e)\n",
        "    i += 1\n",
        "    \n",
        "  y = set(pdf[pdf.columns[1]].tolist())\n",
        "  dicY = {}\n",
        "  # i = len(labels)\n",
        "  for e in y:\n",
        "    if(e in dicX):\n",
        "      dicY[e] = dicX[e]\n",
        "    else:\n",
        "      dicY[e] = i\n",
        "      i += 1\n",
        "    labels.append(e)\n",
        "\n",
        "  fig = go.Figure(data=[go.Sankey(\n",
        "    node = dict(\n",
        "      thickness = 5,\n",
        "      line = dict(color = \"green\", width = 0.1),\n",
        "      label = labels,\n",
        "      color = \"blue\"\n",
        "    ),\n",
        "    link = dict(\n",
        "      # indices correspond to labels\n",
        "      source = [dicX[e] for e in pdf[pdf.columns[0]].tolist()],\n",
        "      target = [dicY[e] for e in pdf[pdf.columns[1]].tolist()],\n",
        "      value = pdf[pdf.columns[2]].tolist()\n",
        "  ))])\n",
        "\n",
        "  # showing the plot\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtZl2WZvjtqy"
      },
      "source": [
        "## Récupération du jeu de données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KdpjxvkjxQM"
      },
      "source": [
        "Téléchargement du jeu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "v95gxMsAJxIO",
        "outputId": "3a1ab2e7-71ab-405c-886d-b6960701ebea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 4393M  100 4393M    0     0   131M      0  0:00:33  0:00:33 --:--:--  195M\n"
          ]
        }
      ],
      "source": [
        "!curl -L -o ecommerce-behavior-data-from-multi-category-store.zip 'https://drive.google.com/u/0/uc?id=1CVhmxsU3GY0FYGS1uP3m_tGbyGjEfuQc&export=download&confirm=t'\n",
        "#!curl -L -o ecommerce-behavior-data-from-multi-category-store.zip 'https://storage.googleapis.com/kaggle-data-sets/411512/835452/compressed/2019-Nov.csv.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230128%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230128T111731Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2afcc1a011c86d89fe64d1c12bc1432f703c525f12c74f43fad4f455a5183c74589932fbabb73bce85de91427906abecec18c6929a894fd0ca8657683b665379deea648ef51f6bb4c114125998ee24b7fdd2b630cdc327e142d0f8130f2f5e9306d45293940e87b2c05aa32151f52ab4a85638d5920e6de0fbf13b8daaffd7fbeb21009fc42c8baf268a399a1419b0bf0c9a5a5150732d0d10d4a1b90c7b516d60a01ffb2dc3b42c9266f3acdecf42b791a074f379ec89295af92a337d89af4f092e6a74db6b74f75305604e9593e265dafdf6e25dbe9b9160840864260541f1a188473fa9c59514fd0d4136cd04066084275d95e238525b94333cac9a6b6ceb'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2uCCY-bkC83"
      },
      "source": [
        "Extraction des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QMZvksQ4gAMH",
        "outputId": "a56246fa-f1a8-4b7f-cc47-fd43bfe6bc5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ecommerce-behavior-data-from-multi-category-store.zip\n",
            "  inflating: 2019-Nov.csv            \n",
            "  inflating: 2019-Oct.csv            \n",
            "2019-Nov.csv\n",
            "2019-Oct.csv\n",
            "ecommerce-behavior-data-from-multi-category-store.zip\n",
            "sample_data\n",
            "spark-3.3.1-bin-hadoop3\n",
            "spark-3.3.1-bin-hadoop3.tgz\n"
          ]
        }
      ],
      "source": [
        "!unzip -o ecommerce-behavior-data-from-multi-category-store.zip\n",
        "!ls ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUYJ9R3aj-DL"
      },
      "source": [
        "Aperçu de format des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEOqnQdykGvr"
      },
      "source": [
        "Chargement du jeu de données dans Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PP8Sb2g_NwZj"
      },
      "outputs": [],
      "source": [
        "#!head -10 2019-Nov.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NkF2bSVljVTm"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"2019-Nov.csv\", header=True, sep=',')\n",
        "#display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5EBOlXXkK6u"
      },
      "source": [
        "Affichage du nombre d'enregistrements du jeu de données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ7A5JrnkQZS"
      },
      "source": [
        "Affichage du schéma du Dataframe Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jPMar0nxsEm8"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzRTNlFlkXW9"
      },
      "source": [
        "Casting de certaines colonnes aux types de données attendus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Nz-RYB5HsK86"
      },
      "outputs": [],
      "source": [
        "df = df.withColumn(\"event_time\",df.event_time.cast(TimestampType()))\n",
        "df = df.withColumn(\"product_id\",df.product_id.cast(IntegerType()))\n",
        "df = df.withColumn(\"category_id\",df.category_id.cast(IntegerType()))\n",
        "df = df.withColumn(\"price\",df.price.cast(DoubleType()))\n",
        "df = df.withColumn(\"user_id\",df.user_id.cast(IntegerType()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsRzcWl6kdtC"
      },
      "source": [
        "Affichage du nouveau schéma du Dataframe Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GATNQ7gusulc"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyHcPe8JkjmJ"
      },
      "source": [
        "Matérialisation du dataframe comme une vue SQL avec la vue <mark>purchases</mark> qui pointe sur lui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-tfrmaQ1lxmP"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView('events')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irFoVU91EpbT"
      },
      "source": [
        "# 1. Le chiffre d'affaire réalisé selon les jours de la semaine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VHKpXkglFu5y"
      },
      "outputs": [],
      "source": [
        "dayNames = ['', 'Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche']\n",
        "  \n",
        "def weekDayName(dayID):\n",
        "  global dayNames\n",
        "  val = int(dayID)\n",
        "  if(1 <= val <= 7):\n",
        "    return dayNames[val]\n",
        "  else:\n",
        "    return \"Unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7Nsz1JT9Fyse",
        "outputId": "14fc9a13-6a10-45e4-dfc1-52ffe0749a66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.weekDayName(dayID)>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "spark.udf.register(\"weekDayName\", weekDayName, StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6-bJPN1EiA6"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT weekDayName(date_format(event_time, 'F')) AS JoursSemaine, SUM(price) AS ChiffreAffaire FROM events WHERE event_type= 'purchase' GROUP BY weekDayName(date_format(event_time, 'F'))\"\n",
        "drawHistogram(sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWrd2DH-2Xal"
      },
      "source": [
        "# 2. Donnons l'évolution du nombre d'achats de produits selon les jours du mois."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extractDay(day):\n",
        "  day1 = str(day).strip().split(\" \")\n",
        "  if len(day1) == 2:\n",
        "    Date, Heure = day1\n",
        "    return Date"
      ],
      "metadata": {
        "id": "ltp2kuEo_YQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.udf.register(\"extractDay\", extractDay, StringType())"
      ],
      "metadata": {
        "id": "pVVCMM_p_10P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oHlvWW12s2F"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT extractDay(event_time) AS JoursMois, COUNT(event_type) FROM events WHERE event_type='purchase' GROUP BY extractDay(event_time)\"\n",
        "drawLine(sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Donnons le top 3 des catégories de produits et leurs chiffres d'affaires par type d'évènement."
      ],
      "metadata": {
        "id": "kzgUb0DXCU5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "CREATE OR REPLACE TEMP VIEW TopProd AS SELECT category_code, event_type, SUM(price) AS chiffreDaffaire FROM events WHERE category_code != 'None' GROUP BY category_code, event_type"
      ],
      "metadata": {
        "id": "vA3PIU8VKZ68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "CREATE OR REPLACE TEMP VIEW TopProd2 AS SELECT category_code, event_type, chiffreDaffaire, RANK() over (PARTITION BY event_type ORDER BY chiffreDaffaire DESC) AS rang FROM TopProd"
      ],
      "metadata": {
        "id": "rvFFWYlJLMZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Donnons le top 3 des catégories de produits et leurs chiffres d'affaires par type d'évènement\n",
        "sql = \"SELECT event_type, category_code, chiffreDaffaire FROM TopProd2 WHERE rang <= 3\"\n",
        "drawHeatmap(sql, scale=math.log)"
      ],
      "metadata": {
        "id": "MGcihL1BMK4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Donnons le chiffre d'affaire gagné sur chaque marque (brand) selon le jour de la semaine."
      ],
      "metadata": {
        "id": "6DqJjSoV0qLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SELECT brand AS Marque, weekDayName(date_format(event_time, 'F')) AS JoursSemaine, SUM(price) AS chiffreDaffaire FROM events GROUP BY brand, weekDayName(date_format(event_time, 'F'))"
      ],
      "metadata": {
        "id": "JsfoYtfhUabW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = \"SELECT weekDayName(date_format(event_time, 'F')) AS JoursSemaine, brand AS Marque, SUM(price) AS chiffreDaffaire FROM events GROUP BY weekDayName(date_format(event_time, 'F')), brand\"\n",
        "drawHeatmap(sql, scale=math.log)"
      ],
      "metadata": {
        "id": "UvN0p2Jt0pJq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy9XjYoebFW+TvOGgqDLqz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}